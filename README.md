# STaR2025

This repository contains the supplemental material for the paper:

**STaR: A Self-Triggered Retrieval-Enhanced Framework for Cost-Efficient Generation with LLMs**

## Overview

STaR is a plug-and-play framework designed to enhance large language model (LLM) generation through dynamic, cost-aware retrieval. It features a lightweight, prompt-level pipeline for self-triggered decision-making, query construction, and evidence integration.

The framework does not require model fine-tuning and is compatible with various black-box LLM APIs.

## Repository Structure

- Scripts for launching experiments
- Configuration files for different datasets
- Documentation and instructions for reproducing results

## Supported Benchmarks

Experiments are conducted on the following datasets:
- 2WikiMultihopQA
- HotpotQA
- StrategyQA
- IIRC

## Reproducibility

To ensure reproducibility:
- All evaluation scripts, configurations, and required components are included.
- Results reported in the paper can be reproduced using the provided materials.
- No fine-tuning is necessary; experiments rely on publicly available quantized LLM models.

## Contact

This repository is part of an anonymous submission. All materials are provided solely for review purposes.
